# building_nano_gpt


## Overview
The primary goal of this project is to build a minimal yet functional GPT model from scratch using PyTorch. The project is an ongoing endeavor that will eventually incorporate the implementation of a non-trivial research paper, further expanding the complexity and depth of the project. By doing so, I aim to demonstrate my ability to dive deep into a specific topic and showcase my expertise in transformer-based models.



## Features:
- Nano GPT Model: A simple but functional implementation of a GPT-like model.
- Character-Level Language Modeling: Training the model on text data at the character level to predict the next character in a sequence.
- Tokenizer Implementation: Custom character encoding and decoding functions.
- Training Loop: Optimized using AdamW with training and validation loss monitoring.
- Text Generation: Text generation function to sample from the trained model.


## Inspiration
This project was inspired by the [YouTube video by Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=13), which provides a detailed walkthrough of implementing a simple transformer model. His work and explanations laid the foundation for building this minimal GPT model.
