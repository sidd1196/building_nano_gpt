# building_nano_gpt


## Overview
The primary goal of this project is to build a minimal yet functional GPT model from scratch using PyTorch. The project is an ongoing endeavor that will eventually incorporate the implementation of a non-trivial research paper, further expanding the complexity and depth of the project. By doing so, I aim to demonstrate my ability to dive deep into a specific topic and showcase my expertise in transformer-based models.



## Features:
- Nano GPT Model: A simple but functional implementation of a GPT-like model.
- Character-Level Language Modeling: Training the model on text data at the character level to predict the next character in a sequence.
- Tokenizer Implementation: Custom character encoding and decoding functions.
- Training Loop: Optimized using AdamW with training and validation loss monitoring.
- Text Generation: Text generation function to sample from the trained model.
